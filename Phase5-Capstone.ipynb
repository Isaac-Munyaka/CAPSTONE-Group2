
Project Proposal: Advanced NLP-based Job Recommendation System for BrighterMonday
CAPSTONE PROJECT
PHASE 5:GROUP 2 MEMBERS:

Isaac Munyaka.
Phemina Wambui.
Otiende Ogada.
Caroline Gesaka.
Ann Njoroge.
Joan Maina.
Background Story
BrighterMonday is a prominent job portal in Kenya, renowned for bridging the gap between job seekers and employers. Despite its success in providing job listings and basic job matching functionalities, there is an increasing demand for more sophisticated and tailored job recommendation systems. These advanced systems aim to enhance the job search experience by offering personalized recommendations that align more closely with individual skills, preferences, and career goals. This evolution is crucial for improving the efficiency and effectiveness of job matching, ultimately benefiting both job seekers and employers by making the process more targeted and relevant (BrighterMonday, 2024).

Introduction
To elevate BrighterMonday’s job matching capabilities and enhance user experience, this project proposes the development of an advanced job recommendation system powered by Natural Language Processing (NLP). By leveraging state-of-the-art NLP technologies, the proposed system aims to deliver highly accurate job recommendations tailored to individual job seekers' skills, preferences, and career aspirations. Additionally, the system will provide comprehensive job insights and actionable feedback, further supporting job seekers in their search and application processes. This advancement will not only improve the relevance of job matches but also empower users with valuable information and guidance, ultimately enhancing their overall experience on the platform.

Business Understanding
The goal of this project is to significantly enhance the accuracy and relevance of job matching on BrighterMonday, ultimately improving the job search experience for seekers and streamlining the recruitment process for employers. By implementing an advanced NLP-based recommendation system, the project aims to provide job seekers with tailored job opportunities that align closely with their skills, preferences, and career goals. For employers, the system will facilitate the identification of qualified candidates by matching job postings with the most suitable applicants. This dual focus will not only help job seekers find better employment opportunities but also assist employers in efficiently sourcing candidates who meet their specific requirements.

Problem Statement
Job seekers on BrighterMonday frequently encounter difficulties in finding job opportunities that closely align with their skills and experience. The existing job matching system relies predominantly on keyword-based searches, which can result in inaccurate job matches and overlooked opportunities. This approach often fails to capture the nuanced qualifications and preferences of job seekers, leading to suboptimal job recommendations and a less efficient job search process. Consequently, there is a need for a more advanced and precise recommendation system that better understands and matches the diverse qualifications and aspirations of job seekers.

Objectives
1.Develop an Advanced NLP-Based Job Recommendation System: Implement cutting-edge Natural Language Processing (NLP) models to enhance the accuracy and relevance of job matching, ensuring that job seekers receive recommendations that are closely aligned with their skills, experience, and career goals.

2.Provide Detailed Job Information: Deliver comprehensive insights into job opportunities, including key details such as compensation, location, work type, and experience level, to help job seekers make informed decisions.

3.Implement Skills Gap Analysis: Develop a mechanism to identify gaps between job seekers' current skills and those required for desired positions, and offer recommendations for skill development to enhance their qualifications and employability.

4.Integrate a User Feedback System: Establish a system for collecting and analyzing user feedback on job recommendations, enabling continuous refinement and improvement of the recommendation engine based on real user experiences and preferences.

5.Offer Job Market Trends and Insights: Provide data-driven insights into current job market trends and in-demand skills, helping job seekers and employers stay informed about evolving industry requirements and opportunities.

Target Audience
Job Seekers: Individuals seeking suitable job opportunities and career growth.

Employers: Companies looking to recruit qualified candidates for job openings.

Recruitment Agencies: Agencies that assist job seekers and employers in the recruitment process.

1.Data Understanding
Dataset: LinkedIn Job Postings

Source: LinkedIn Job Postings (2023 - 2024) (kaggle.com)

Columns:

job_id: Unique identifier for each job posting, essential for tracking and referencing individual jobs.

company_name: Provides the name of the company offering the job, which helps in understanding the job context and employer branding.

title: The job title is critical for identifying the nature of the job and matching it with user preferences.

description: Contains detailed information about job responsibilities and requirements, which is crucial for matching the job with user skills and interests.

max_salary: Indicates the highest salary offered for the job, helping to match job seekers' salary expectations.

min_salary: Shows the lowest salary offered, providing a range for matching job seekers' financial requirements.

location: Geographic location of the job, which is important for matching based on job seekers' preferred or available locations.

views: Number of views the job posting has received, which can indicate the popularity or competitiveness of the job.

med_salary: Median salary for the job, providing a central measure of compensation, useful for understanding typical earnings.

applies: Number of applications received, which can reflect job demand and help gauge the job's attractiveness.

remote_allowed: Indicates if remote work is an option, which is increasingly relevant for job seekers preferring or needing remote work arrangements.

formatted_experience_level: Specifies the required experience level (e.g., entry-level, senior), aiding in matching jobs with job seekers' experience.

skills_desc: Describes the skills required for the job, crucial for aligning job seekers' skills with job requirements.

listed_time: Timestamp of when the job was posted, helping to understand the job’s recency and relevance.

posting_domain: Indicates the industry or sector of the job, useful for matching jobs with job seekers’ industry interests.

currency: Specifies the currency in which the salary is offered, important for job seekers in different regions or countries.

compensation_type: Details the type of compensation (e.g., base salary, bonuses), helping job seekers understand the total compensation package.

Load the DataSet
import pandas as pd

file_path = r'C:\Users\Caro\Downloads\postings.csv'

try:
    data = pd.read_csv(file_path, encoding='utf-8', on_bad_lines='warn')
except UnicodeDecodeError:
    data = pd.read_csv(file_path, encoding='latin1', on_bad_lines='warn')

data.head()
job_id	company_name	title	description	max_salary	pay_period	location	company_id	views	med_salary	...	expiry	closed_time	formatted_experience_level	skills_desc	listed_time	posting_domain	sponsored	work_type	currency	compensation_type
0	921716	Corcoran Sawyer Smith	Marketing Coordinator	Job descriptionA leading real estate firm in N...	20.0	HOURLY	Princeton, NJ	2774458.0	20.0	NaN	...	1.715990e+12	NaN	NaN	Requirements: \n\nWe are seeking a College or ...	1.713398e+12	NaN	0	FULL_TIME	USD	BASE_SALARY
1	1829192	NaN	Mental Health Therapist/Counselor	At Aspen Therapy and Wellness , we are committ...	50.0	HOURLY	Fort Collins, CO	NaN	1.0	NaN	...	1.715450e+12	NaN	NaN	NaN	1.712858e+12	NaN	0	FULL_TIME	USD	BASE_SALARY
2	10998357	The National Exemplar	Assitant Restaurant Manager	The National Exemplar is accepting application...	65000.0	YEARLY	Cincinnati, OH	64896719.0	8.0	NaN	...	1.715870e+12	NaN	NaN	We are currently accepting resumes for FOH - A...	1.713278e+12	NaN	0	FULL_TIME	USD	BASE_SALARY
3	23221523	Abrams Fensterman, LLP	Senior Elder Law / Trusts and Estates Associat...	Senior Associate Attorney - Elder Law / Trusts...	175000.0	YEARLY	New Hyde Park, NY	766262.0	16.0	NaN	...	1.715488e+12	NaN	NaN	This position requires a baseline understandin...	1.712896e+12	NaN	0	FULL_TIME	USD	BASE_SALARY
4	35982263	NaN	Service Technician	Looking for HVAC service tech with experience ...	80000.0	YEARLY	Burlington, IA	NaN	3.0	NaN	...	1.716044e+12	NaN	NaN	NaN	1.713452e+12	NaN	0	FULL_TIME	USD	BASE_SALARY
5 rows × 28 columns

data.tail()
job_id	company_name	title	description	max_salary	pay_period	location	company_id	views	med_salary	...	expiry	closed_time	formatted_experience_level	skills_desc	listed_time	posting_domain	sponsored	work_type	currency	compensation_type
123844	3906267117	Lozano Smith	Title IX/Investigations Attorney	Our Walnut Creek office is currently seeking a...	195000.0	YEARLY	Walnut Creek, CA	56120.0	1.0	NaN	...	1.716163e+12	NaN	Mid-Senior level	NaN	1.713571e+12	NaN	0	FULL_TIME	USD	BASE_SALARY
123845	3906267126	Pinterest	Staff Software Engineer, ML Serving Platform	About Pinterest:\n\nMillions of people across ...	NaN	NaN	United States	1124131.0	3.0	NaN	...	1.716164e+12	NaN	Mid-Senior level	NaN	1.713572e+12	www.pinterestcareers.com	0	FULL_TIME	NaN	NaN
123846	3906267131	EPS Learning	Account Executive, Oregon/Washington	Company Overview\n\nEPS Learning is a leading ...	NaN	NaN	Spokane, WA	90552133.0	3.0	NaN	...	1.716164e+12	NaN	Mid-Senior level	NaN	1.713572e+12	epsoperations.bamboohr.com	0	FULL_TIME	NaN	NaN
123847	3906267195	Trelleborg Applied Technologies	Business Development Manager	The Business Development Manager is a 'hunter'...	NaN	NaN	Texas, United States	2793699.0	4.0	NaN	...	1.716165e+12	NaN	NaN	NaN	1.713573e+12	NaN	0	FULL_TIME	NaN	NaN
123848	3906267224	Solugenix	Marketing Social Media Specialist	Marketing Social Media Specialist - 
75...	75000.0	YEARLY	San Juan Capistrano, CA	43325.0	2.0	NaN	...	1.716165e+12	NaN	Mid-Senior level	NaN	1.713573e+12	NaN	0	FULL_TIME	USD	BASE_SALARY
5 rows × 28 columns

data.description
0         Job descriptionA leading real estate firm in N...
1         At Aspen Therapy and Wellness , we are committ...
2         The National Exemplar is accepting application...
3         Senior Associate Attorney - Elder Law / Trusts...
4         Looking for HVAC service tech with experience ...
                                ...                        
123844    Our Walnut Creek office is currently seeking a...
123845    About Pinterest:\n\nMillions of people across ...
123846    Company Overview\n\nEPS Learning is a leading ...
123847    The Business Development Manager is a 'hunter'...
123848    Marketing Social Media Specialist - $70k – $75...
Name: description, Length: 123849, dtype: object
data.shape
(123849, 28)
data.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 123849 entries, 0 to 123848
Data columns (total 28 columns):
 #   Column                      Non-Null Count   Dtype  
---  ------                      --------------   -----  
 0   job_id                      123849 non-null  int64  
 1   company_name                122130 non-null  object 
 2   title                       123849 non-null  object 
 3   description                 123842 non-null  object 
 4   max_salary                  29793 non-null   float64
 5   pay_period                  36073 non-null   object 
 6   location                    123849 non-null  object 
 7   company_id                  122132 non-null  float64
 8   views                       122160 non-null  float64
 9   med_salary                  6280 non-null    float64
 10  min_salary                  29793 non-null   float64
 11  formatted_work_type         123849 non-null  object 
 12  applies                     23320 non-null   float64
 13  original_listed_time        123849 non-null  float64
 14  remote_allowed              15246 non-null   float64
 15  job_posting_url             123849 non-null  object 
 16  application_url             87184 non-null   object 
 17  application_type            123849 non-null  object 
 18  expiry                      123849 non-null  float64
 19  closed_time                 1073 non-null    float64
 20  formatted_experience_level  94440 non-null   object 
 21  skills_desc                 2439 non-null    object 
 22  listed_time                 123849 non-null  float64
 23  posting_domain              83881 non-null   object 
 24  sponsored                   123849 non-null  int64  
 25  work_type                   123849 non-null  object 
 26  currency                    36073 non-null   object 
 27  compensation_type           36073 non-null   object 
dtypes: float64(11), int64(2), object(15)
memory usage: 26.5+ MB
Unique Values
# Display unique values in formatted_experience_level
data['formatted_experience_level'].unique()
array([nan, 'Entry level', 'Mid-Senior level', 'Associate', 'Director',
       'Internship', 'Executive'], dtype=object)
# Display unique values in formatted_work_type
data['formatted_work_type'].unique()
array(['Full-time', 'Internship', 'Contract', 'Part-time', 'Temporary',
       'Volunteer', 'Other'], dtype=object)
# Display unique values in job titles
data['title'].unique()
array(['Marketing Coordinator', 'Mental Health Therapist/Counselor',
       'Assitant Restaurant Manager', ...,
       'Staff Software Engineer, ML Serving Platform',
       'Account Executive, Oregon/Washington',
       'Marketing Social Media Specialist'], dtype=object)
# Display unique values in location 
data['location'].unique()
array(['Princeton, NJ', 'Fort Collins, CO', 'Cincinnati, OH', ...,
       'Middlebury, IN', 'Liberty Corner, NJ', 'Carroll County, MD'],
      dtype=object)
# Display unique values in work_type
data['work_type'].unique()
array(['FULL_TIME', 'INTERNSHIP', 'CONTRACT', 'PART_TIME', 'TEMPORARY',
       'VOLUNTEER', 'OTHER'], dtype=object)
# Display unique values in remote_allowed
data['remote_allowed'].unique()
array([nan,  1.])
# Display unique values in the pay_period column
data['pay_period'].unique()
array(['HOURLY', 'YEARLY', nan, 'MONTHLY', 'WEEKLY', 'BIWEEKLY'],
      dtype=object)
data['company_name'].unique()
array(['Corcoran Sawyer Smith', nan, 'The National Exemplar ', ...,
       'The Dyrt', 'EPS Learning', 'Trelleborg Applied Technologies'],
      dtype=object)
data['currency'].unique()
array(['USD', nan, 'CAD', 'BBD', 'EUR', 'AUD', 'GBP'], dtype=object)
data['compensation_type'].unique()
array(['BASE_SALARY', nan], dtype=object)
Duplicates
# Check for duplicates based on all columns
duplicate_rows = data[data.duplicated()]

# Display the duplicate rows, if any
if not duplicate_rows.empty:
    print("Duplicate Rows:")
    print(duplicate_rows)
else:
    print("No duplicate rows found.")
No duplicate rows found.
PlaceHolders
#Define a comprehensive list of potential placeholder values
common_placeholders = ["", "na", "n/a", "NaN", "none", "null", "-", "--", "?", "??", "unknown", "missing", "void"]
# Loop through each column and check for potential placeholders
found_placeholder = False
for column in data.columns:
    unique_values = data[column].unique()
    for value in unique_values:
        if pd.isna(value) or (isinstance(value, str) and value.strip().lower() in common_placeholders):
            count = (data[column] == value).sum()
            print(f"Column '{column}': Found {count} occurrences of potential placeholder '{value}'")
            found_placeholder = True
if not found_placeholder:
    print("No potential placeholders found in the DataFrame.")
Column 'company_name': Found 0 occurrences of potential placeholder 'nan'
Column 'description': Found 0 occurrences of potential placeholder 'nan'
Column 'max_salary': Found 0 occurrences of potential placeholder 'nan'
Column 'pay_period': Found 0 occurrences of potential placeholder 'nan'
Column 'company_id': Found 0 occurrences of potential placeholder 'nan'
Column 'views': Found 0 occurrences of potential placeholder 'nan'
Column 'med_salary': Found 0 occurrences of potential placeholder 'nan'
Column 'min_salary': Found 0 occurrences of potential placeholder 'nan'
Column 'applies': Found 0 occurrences of potential placeholder 'nan'
Column 'remote_allowed': Found 0 occurrences of potential placeholder 'nan'
Column 'application_url': Found 0 occurrences of potential placeholder 'nan'
Column 'closed_time': Found 0 occurrences of potential placeholder 'nan'
Column 'formatted_experience_level': Found 0 occurrences of potential placeholder 'nan'
Column 'skills_desc': Found 0 occurrences of potential placeholder 'nan'
Column 'posting_domain': Found 0 occurrences of potential placeholder 'nan'
Column 'currency': Found 0 occurrences of potential placeholder 'nan'
Column 'compensation_type': Found 0 occurrences of potential placeholder 'nan'
Missing Values
#check missing values
data.isnull().sum()
job_id                             0
company_name                    1719
title                              0
description                        7
max_salary                     94056
pay_period                     87776
location                           0
company_id                      1717
views                           1689
med_salary                    117569
min_salary                     94056
formatted_work_type                0
applies                       100529
original_listed_time               0
remote_allowed                108603
job_posting_url                    0
application_url                36665
application_type                   0
expiry                             0
closed_time                   122776
formatted_experience_level     29409
skills_desc                   121410
listed_time                        0
posting_domain                 39968
sponsored                          0
work_type                          0
currency                       87776
compensation_type              87776
dtype: int64
#calcute the  percentage of missing values
data.isnull().sum()*100 /len(data)
job_id                         0.000000
company_name                   1.387981
title                          0.000000
description                    0.005652
max_salary                    75.944093
pay_period                    70.873402
location                       0.000000
company_id                     1.386366
views                          1.363757
med_salary                    94.929309
min_salary                    75.944093
formatted_work_type            0.000000
applies                       81.170619
original_listed_time           0.000000
remote_allowed                87.689848
job_posting_url                0.000000
application_url               29.604599
application_type               0.000000
expiry                         0.000000
closed_time                   99.133622
formatted_experience_level    23.745852
skills_desc                   98.030666
listed_time                    0.000000
posting_domain                32.271556
sponsored                      0.000000
work_type                      0.000000
currency                      70.873402
compensation_type             70.873402
dtype: float64
Column Types
# column names
data.columns
Index(['job_id', 'company_name', 'title', 'description', 'max_salary',
       'pay_period', 'location', 'company_id', 'views', 'med_salary',
       'min_salary', 'formatted_work_type', 'applies', 'original_listed_time',
       'remote_allowed', 'job_posting_url', 'application_url',
       'application_type', 'expiry', 'closed_time',
       'formatted_experience_level', 'skills_desc', 'listed_time',
       'posting_domain', 'sponsored', 'work_type', 'currency',
       'compensation_type'],
      dtype='object')
# Identify numerical columns (int, float, datetime types)
numerical_columns = data.select_dtypes(include=['number', 'datetime']).columns.tolist()

print("Numerical Columns:", numerical_columns)
Numerical Columns: ['job_id', 'max_salary', 'company_id', 'views', 'med_salary', 'min_salary', 'applies', 'original_listed_time', 'remote_allowed', 'expiry', 'closed_time', 'listed_time', 'sponsored']
# Identify categorical columns
categorical_columns = [col for col in data.select_dtypes(include=['object']).columns if col in [
    'pay_period', 'formatted_work_type', 'application_type', 'formatted_experience_level', 
    'work_type', 'currency', 'compensation_type']]

print("Categorical Columns:", categorical_columns)
Categorical Columns: ['pay_period', 'formatted_work_type', 'application_type', 'formatted_experience_level', 'work_type', 'currency', 'compensation_type']
# Identify string columns
string_columns = [col for col in data.select_dtypes(include=['object']).columns if col not in categorical_columns]

print("String Columns:", string_columns)
String Columns: ['company_name', 'title', 'description', 'location', 'job_posting_url', 'application_url', 'skills_desc', 'posting_domain']
2.Data Cleaning
Handling Missing Values
Columns to Drop
# Drop the specified columns with reasons
columns_to_drop = [
    'med_salary',      # 94.93% missing; does not provide additional insights from max/min salary
    'company_id',      # Redundant if company_name is sufficient
    'closed_time',      # 99.13% missing; not critical to the core objectives
    'application_url',    # Not integrating a direct application feature 
    'job_posting_url',    # Not integrating a direct application feature
    'posting_domain',    #

]

# Dropping the columns
# original listed time(based that we have listed time column,it might be redundant)
# sponsored(not essential for recommendation logic but useful for marketting analysis)
#application_type(useful for analysis for application process but not for recommender logic)
#formatted work type and work type(appears to be redundant;choose one that is useful for analysis)
#expiry(can be used for filtering jobs based on expiry but not for recommendation logic)
#'application_url(Not integrating a direct application feature) 
#'job_posting_url(Not integrating a direct application feature)

data =data.drop(columns=columns_to_drop)
1.Numerical columns
# Filled with mean values to maintain the average trends
# Impute numerical columns with mean or median
numerical_columns = ['job_id','views', 'applies','original_listed_time', 'remote_allowed', 'expiry', 'sponsored']
for col in numerical_columns:
    data[col].fillna(data[col].mean(), inplace=True)
1.1.Numerical Timeseries columns
# Interpolation to estimate missing values in a time series context.
data['max_salary'] = data['max_salary'].interpolate()
data['min_salary'] = data['min_salary'].interpolate()
2.Categorical columns
#Filled with the mode to maintain the most common category
categorical_columns = ['company_name','formatted_experience_level', 'currency']
for col in categorical_columns:
    data[col].fillna(data[col].mode()[0], inplace=True)
3.Text columns
#Filled with placeholders to avoid null values.
# Fill text columns with placeholders
data['description'].fillna('No description provided', inplace=True)
data['skills_desc'].fillna('No skills provided', inplace=True)

# Fill missing values with a placeholder
data['company_name'].fillna('Unknown Company', inplace=True)

# Fill missing values with a placeholder
data['pay_period'].fillna('Not Provided', inplace=True)

# Fill missing values with a placeholder
data['compensation_type'].fillna('Not Specified', inplace=True)
3.Boolen columns
# Missing values are treated as False.
# convert to Binary
data['remote_allowed'] = data['remote_allowed'].fillna(False).astype(bool)
4. Date/Time columns
# Convert to datetime
data['listed_time'] = pd.to_datetime(data['listed_time'])

# Forward fill missing 'listed_time' values
data['listed_time'].fillna(method='ffill', inplace=True)

# Reset index to ensure 'closed_time' is back to column
data.reset_index(inplace=True)

# Set 'listed_time' as index for time series analysis
data.set_index('listed_time', inplace=True)

#check missing values after cleaning
data.isnull().sum()
index                         0
job_id                        0
company_name                  0
title                         0
description                   0
max_salary                    0
pay_period                    0
location                      0
views                         0
min_salary                    0
formatted_work_type           0
applies                       0
original_listed_time          0
remote_allowed                0
application_type              0
expiry                        0
formatted_experience_level    0
skills_desc                   0
sponsored                     0
work_type                     0
currency                      0
compensation_type             0
dtype: int64
#check original dataframe columns
data.columns

#Assingn cleaned dataframe 
df = data

#copy original dataframe original DataFrame
df = df_copy = df.copy()

# Save the copied DataFrame to a CSV file
df_copy.to_csv('copied_postings.csv', index=False)
df.columns
Index(['index', 'job_id', 'company_name', 'title', 'description', 'max_salary',
       'pay_period', 'location', 'views', 'min_salary', 'formatted_work_type',
       'applies', 'original_listed_time', 'remote_allowed', 'application_type',
       'expiry', 'formatted_experience_level', 'skills_desc', 'sponsored',
       'work_type', 'currency', 'compensation_type'],
      dtype='object')
df.shape
(123849, 22)
3.Exploratory Data Analysis
3.1.Univariet Analysis
1.Job Title Distribution
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

plt.figure(figsize=(10, 6))
sns.countplot(x='title', data=df, order=df['title'].value_counts().index[:10])
plt.xticks(rotation=45, ha='right')
plt.title('Top 10 Job Titles Distribution')
plt.xlabel('Job Title')
plt.ylabel('Count')
plt.tight_layout()

2.Company Distribution
plt.figure(figsize=(10, 6))
sns.countplot(x='company_name', data=df, order=df['company_name'].value_counts().index[:10])
plt.xticks(rotation=45, ha='right')
plt.title('Top 10 Companies by Job Postings')
plt.xlabel('Company Name')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

3.Salary Distribution
plt.figure(figsize=(8, 6))
sns.histplot(df['max_salary'], bins=20, kde=True, label='Maximum Salary', color='blue', alpha=0.7)
sns.histplot(df['min_salary'], bins=20, kde=True, label='Minimum Salary', color='red', alpha=0.7)
plt.title('Distribution of Salary (Max and Min)')
plt.xlabel('Salary')
plt.ylabel('Frequency')
plt.legend()
plt.tight_layout()
plt.show()

4.Location Distribution
plt.figure(figsize=(12, 6))
sns.countplot(x='location', data=df, order=df['location'].value_counts().index[:10])
plt.xticks(rotation=45, ha='right')
plt.title('Top 10 Job Locations')
plt.xlabel('Location')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

5.Work_Type Distribution
plt.figure(figsize=(8, 6))
sns.countplot(x='formatted_work_type', data=df)
plt.xticks(rotation=45, ha='right')
plt.title('Distribution of Work Types')
plt.xlabel('Work Type')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

6.Experience Level Distribution
plt.figure(figsize=(8, 6))
sns.countplot(x='formatted_experience_level', data=df)
plt.xticks(rotation=45, ha='right')
plt.title('Distribution of Experience Levels')
plt.xlabel('Experience Level')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

7.Application Type Distribution
plt.figure(figsize=(8, 6))
sns.countplot(x='application_type', data=df)
plt.xticks(rotation=45, ha='right')
plt.title('Distribution of Application Types')
plt.xlabel('Application Type')
plt.ylabel('Count')
plt.tight_layout()
plt.show()


8.Currency Distribution
plt.figure(figsize=(8, 6))
sns.countplot(x='currency', data=df)
plt.xticks(rotation=45, ha='right')
plt.title('Distribution of Currencies')
plt.xlabel('Currency')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

3.2.Bivariet Analysis
1.Top 20 Job Titles with Most Applications
# Group by 'title' and sum the 'applies' column
title_applications = df.groupby('title')['applies'].sum().reset_index()

# Sort the results in descending order of applications
title_applications_sorted = title_applications.sort_values(by='applies', ascending=False)

# Select the top 10 job titles with the most applications
top_titles = title_applications_sorted.head(20)

# Plot the results
plt.figure(figsize=(14, 8))
sns.barplot(data=top_titles, x='applies', y='title', palette='viridis')
plt.title('Top 20 Job Titles with Most Applications')
plt.xlabel('Number of Applications')
plt.ylabel('Job Title')
plt.show()

2.Pay Period vs Max_salary
# Aggregate the data to get the median salary for each pay period
median_salary_by_pay_period = df.groupby('pay_period')['max_salary'].median().reset_index()

plt.figure(figsize=(8, 6))
sns.barplot(data=median_salary_by_pay_period, x='pay_period', y='max_salary', palette='Set3')
plt.title('Max Salary by Pay Period')
plt.xlabel('Pay Period')
plt.ylabel('Max Salary')
plt.xticks(rotation=45)
plt.show()

3.Top 20 Job Titles with Most Views
# Group by 'title' and sum the 'views' column
title_views = df.groupby('title')['views'].sum().reset_index()

# Sort the results in descending order of views
title_views_sorted = title_views.sort_values(by='views', ascending=False)

# Select the top 10 job titles with the most views
top_titles_views = title_views_sorted.head(20)

# Plot the results
plt.figure(figsize=(14, 8))
sns.barplot(data=top_titles_views, x='views', y='title', palette='viridis')
plt.title('Top 20 Job Titles with Most Views')
plt.xlabel('Number of Views')
plt.ylabel('Job Title')
plt.show()

4.Bottom 20 Job Titles with Least Views
# Group by 'title' and sum the 'views' column
title_views = df.groupby('title')['views'].sum().reset_index()

# Sort the results in ascending order of views
title_views_sorted = title_views.sort_values(by='views', ascending=True)

# Select the bottom 20 job titles with the least views
bottom_titles_views = title_views_sorted.head(20)

# Plot the results
plt.figure(figsize=(14, 10))
sns.barplot(data=bottom_titles_views, x='views', y='title', palette='viridis')
plt.title('Bottom 20 Job Titles with Least Views')
plt.xlabel('Number of Views')
plt.ylabel('Job Title')
plt.show()

5.Average Minimum and Maximum Salaries by Experience Level
# Calculate average minimum and maximum salaries by experience level
average_salaries = df.groupby('formatted_experience_level')[['min_salary', 'max_salary']].mean().sort_values(by='min_salary')

# Plotting
plt.figure(figsize=(10, 6))

# Create a bar plot for minimum salaries
sns.barplot(x=average_salaries['min_salary'], y=average_salaries.index, palette='viridis', label='Minimum Salary', alpha=0.6)

# Create a bar plot for maximum salaries
sns.barplot(x=average_salaries['max_salary'], y=average_salaries.index, palette='magma', label='Maximum Salary', alpha=0.6)

# Adding title and labels
plt.title('Average Minimum and Maximum Salaries by Experience Level')
plt.xlabel('Average Salary')
plt.ylabel('Experience Level')

# Add legend
plt.legend()

# Show plot
plt.show()

6. Max Salary vs Min Salary by Experience level in Top Ten Companies
# Identify the top 10 companies by the number of job postings
top_companies = df['company_name'].value_counts().head(10).index

# Filter the DataFrame to include only these top companies
top_companies_df = df[df['company_name'].isin(top_companies)].copy()

# Ensure salaries are numeric
salary_columns = ['max_salary', 'min_salary']
top_companies_df.loc[:, salary_columns] = top_companies_df.loc[:, salary_columns].apply(pd.to_numeric, errors='coerce')

# Group by company and experience level, then calculate the median salary
grouped_df = top_companies_df.groupby(['company_name', 'formatted_experience_level'])[salary_columns].median().reset_index()

# Sort the DataFrame by max_salary in descending order
grouped_df_max = grouped_df.sort_values(by='max_salary', ascending=False)
# Sort the DataFrame by min_salary in descending order
grouped_df_min = grouped_df.sort_values(by='min_salary', ascending=False)

# Set the visual style of the plots
sns.set(style="whitegrid")

# Plot the results
plt.figure(figsize=(14, 12))

# Function to plot barplots
def plot_salary(df, salary_type, title, position):
    plt.subplot(2, 1, position)
    sns.barplot(data=df, x='company_name', y=salary_type, hue='formatted_experience_level', palette='Set2')
    plt.title(title)
    plt.xlabel('Company Name')
    plt.ylabel(salary_type.capitalize() + ' Salary')
    plt.xticks(rotation=45)
    plt.legend(title='Experience Level', bbox_to_anchor=(1.05, 1), loc='upper left')

# Plot max_salary
plot_salary(grouped_df_max, 'max_salary', 'Max Salary by Experience Level for Top 10 Companies (Ordered by Max Salary)', 1)

# Plot min_salary
plot_salary(grouped_df_min, 'min_salary', 'Min Salary by Experience Level for Top 10 Companies (Ordered by Min Salary)', 2)

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

7.Location vs Jop posting Metrics
plt.figure(figsize=(10, 6))
top_locations = df['location'].value_counts().head(10).index
top_locations_df = df[df['location'].isin(top_locations)]

sns.barplot(data=top_locations_df, x='location', y='views', estimator=np.mean)
plt.title('Average Job Views by Location')
plt.xticks(rotation=45)
plt.show()

8.Remote Allowed vs. Application Type
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='application_type', hue='remote_allowed')
plt.title('Remote Allowed vs. Application Type')
plt.xlabel('Application Type')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.legend(title='Remote Allowed')
plt.show()

9.Average Max Salary by Work Type
plt.figure(figsize=(10, 6))

plt.subplot(1, 3, 1)
summary = df.groupby('formatted_work_type')['min_salary'].mean().sort_values()
sns.barplot(x=summary.index, y=summary.values, palette='viridis')
plt.title('Average Min Salary by Work Type')
plt.xticks(rotation=45)

plt.subplot(1, 3, 3)
summary = df.groupby('formatted_work_type')['max_salary'].mean().sort_values()
sns.barplot(x=summary.index, y=summary.values, palette='viridis')
plt.title('Average Max Salary by Work Type')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

10.Number of Skills vs. Applies'
# Calculate the number of skills for each job posting
df['num_skills'] = df['skills_desc'].apply(lambda x: len(x.split(',')) if pd.notna(x) else 0)

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='num_skills', y='applies', alpha=0.5)
plt.title('Number of Skills vs. Applies')
plt.xlabel('Number of Skills')
plt.ylabel('Number of Applies')
plt.show()

11.Views vs. Applies
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='views', y='applies', alpha=0.5)
plt.title('Views vs. Applies')
plt.xlabel('Views')
plt.ylabel('Applies')
plt.show()

12.Work Type by Remote Allowed
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='formatted_work_type', hue='remote_allowed', palette='Set2')
plt.title('Work Type by Remote Allowed')
plt.xlabel('Work Type')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.legend(title='Remote Allowed')
plt.show()

13.Word Cloud of Job Titles
from wordcloud import WordCloud

# Generatign a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='black').generate(' '.join(df["title"]))
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Job Titles')
plt.show()

14.Word Cloud of Job location
wordcloud = WordCloud(width=800, height=400, background_color='black').generate(' '.join(df["location"]))
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Job location')
plt.show()

15.Word Cloud of Skills Description
wordcloud = WordCloud(width=800, height=400, background_color='black').generate(' '.join(df["skills_desc"]))
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Skills Description')
plt.show()

3.3.Multivariet Analysis
plt.figure(figsize=(10, 6))
correlation_matrix = df[['max_salary', 'min_salary', 'views', 'applies']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix Heatmap')
plt.show()

Correlation Analysis
# Pearson Correlation
correlation_matrix = df[['max_salary', 'min_salary', 'views', 'applies']].corr(method='pearson')
print(correlation_matrix)

# Spearman Rank Correlation
spearman_corr = df[['max_salary', 'min_salary', 'views', 'applies']].corr(method='spearman')
print(spearman_corr)
            max_salary  min_salary     views   applies
max_salary    1.000000    0.996354  0.002545 -0.004098
min_salary    0.996354    1.000000  0.002917 -0.003838
views         0.002545    0.002917  1.000000  0.477376
applies      -0.004098   -0.003838  0.477376  1.000000
            max_salary  min_salary     views   applies
max_salary    1.000000    0.961736  0.033611 -0.015007
min_salary    0.961736    1.000000  0.038761 -0.019681
views         0.033611    0.038761  1.000000 -0.274813
applies      -0.015007   -0.019681 -0.274813  1.000000
Statistical Analysis
3.4.Descriptive Analysis
# Set the display option to show floats as integers
pd.options.display.float_format = '{:,.0f}'.format

# Summary Statistics for Numeric Columns
summary_stats = df.describe()
summary_stats
index	job_id	max_salary	views	min_salary	applies	original_listed_time	expiry	sponsored	num_skills
count	123849.000000	1.238490e+05	1.238490e+05	123849.000000	1.238490e+05	123849.000000	1.238490e+05	1.238490e+05	123849.0	123849.000000
mean	61924.000000	3.896402e+09	9.003333e+04	14.618247	6.342678e+04	10.591981	1.713152e+12	1.716213e+12	0.0	1.049593
std	35752.271082	8.404355e+07	4.248374e+05	85.315824	3.001755e+05	12.604269	4.848209e+08	2.321394e+09	0.0	0.951370
min	0.000000	9.217160e+05	1.000000e+00	1.000000	1.000000e+00	1.000000	1.701811e+12	1.712903e+12	0.0	1.000000
25%	30962.000000	3.894587e+09	2.335133e+04	3.000000	1.653333e+04	10.591981	1.712863e+12	1.715481e+12	0.0	1.000000
50%	61924.000000	3.901998e+09	8.000000e+04	4.000000	6.000000e+04	10.591981	1.713395e+12	1.716042e+12	0.0	1.000000
75%	92886.000000	3.904707e+09	1.300000e+05	8.000000	9.272727e+04	10.591981	1.713478e+12	1.716088e+12	0.0	1.000000
max	123848.000000	3.906267e+09	1.200000e+08	9975.000000	8.500000e+07	967.000000	1.713573e+12	1.729125e+12	0.0	87.000000
# Frequency Distribution for Categorical Columns
company_name_dist = df['company_name'].value_counts()
company_name_dist
company_name
Liberty Healthcare and Rehabilitation Services    2827
The Job Network                                   1003
J. Galt                                            604
TEKsystems                                         529
Lowe's Companies, Inc.                             527
                                                  ... 
IMA AUTOMATION                                       1
Consumer Edge                                        1
Proterra Inc                                         1
Ava Labs                                             1
Trelleborg Applied Technologies                      1
Name: count, Length: 24428, dtype: int64
location_dist = df['location'].value_counts()
location_dist
location
United States          8125
New York, NY           2756
Chicago, IL            1834
Houston, TX            1762
Dallas, TX             1383
                       ... 
Medway, OH                1
Sulphur Springs, IL       1
Derry Village, NH         1
Ocean Grove, NJ           1
Carroll County, MD        1
Name: count, Length: 8526, dtype: int64
experience_level_dist = df['formatted_experience_level'].value_counts()
experience_level_dist
